
# Ex.No: 2 	Evaluation of 2024 Prompting Tools Across Diverse AI Platforms: ChatGPT, Claude, Bard, Cohere Command, and Meta 
### DATE: 07/03/2025                                                                         
### REGISTER NUMBER : 212222040170
 
### Aim:
To compare the performance, user experience, and response quality of different AI platforms (ChatGPT, Claude, Bard, Cohere Command, and Meta) within a specific use case, such as summarizing text or answering technical questions. Generate a Prompt based output using different Prompting tools of 2024.

### AI Tools required:
Platforms: ChatGPT, Claude, Bard, Cohere Command, and Meta

## Algorithm: 

1. **Define the Use Case:**  
   - Choose a technical concept to be explained across platforms (e.g., Basics of Cloud Computing).

2. **Create a Standard Prompt:**  
   - Frame a simple and clear prompt suitable for college students.
   - Ensure that the same prompt is submitted to all AI platforms.

3. **Select the Platforms:**  
   - ChatGPT (GPT-4-turbo), Claude 3 Opus, Bard (Gemini 1.5 Pro), Cohere Command R+, and Meta Llama

4. **Input and Collect Responses:**  
   - Enter the prompt into each AI tool individually.
   - Record outputs in the same conditions (no hints or follow-ups).

5. **Evaluate Each Response:**  
   - Rate on:
     - **Accuracy**
     - **Clarity**
     - **Depth**
     - **Relevance**

6. **Analyze and Compare:**  
   - Tabulate the scores.
   - Identify which platform gives better explanatory output.

---

## Use Case Selected: 
**Explaining a Basic Concept in Cloud Computing**

---

## Standard Prompt Used: 
*"Explain what Cloud Computing is, using simple language and examples that college students can easily understand."*

---

## Platforms Used: 
- ChatGPT (OpenAI GPT-4 Turbo)  
- Claude 3 Opus (Anthropic)  
- Bard (Google Gemini 1.5)  
- Cohere Command R+  
- Meta Llama 3 (70B)

---

## Input and Execution: 
- Each platform received the exact same prompt.
- Responses were collected without additional context or corrections.

---

## Evaluation Metrics: 
- **Accuracy** (Correctness of definition)  
- **Clarity** (Ease of understanding)  
- **Depth** (Presence of examples and elaborations)  
- **Relevance** (Focus on the asked concept)

---

## Collected Responses Summary:

- **ChatGPT**: Delivered an excellent, real-world relatable explanation (e.g., "Google Drive" analogy).  
- **Claude**: Very clear with a step-by-step breakdown of how cloud services work.  
- **Bard**: Focused on key points but missed deep examples.  
- **Cohere Command**: Gave a correct but surface-level explanation without real-world examples.  
- **Meta Llama 3**: Focused more on technical terms; needed a bit simpler explanation for students.

---

## Analysis Table:

| **Platform**        | **Accuracy** | **Clarity** | **Depth** | **Relevance** | **Total (/20)** |
|----------------------|--------------|-------------|-----------|---------------|-----------------|
| ChatGPT              | 5            | 5           | 5         | 5             | 20              |
| Claude               | 5            | 5           | 4         | 5             | 19              |
| Bard (Gemini)        | 4            | 5           | 3         | 4             | 16              |
| Cohere Command       | 4            | 4           | 2         | 4             | 14              |
| Meta (Llama 3 70B)   | 4            | 3           | 3         | 4             | 14              |

---

### Conclusion: 
Thus, the experiment to evaluate the prompting tools was successfully performed.  
**ChatGPT** delivered the most balanced, student-friendly explanation, scoring the highest total. **Claude** also performed excellently but was slightly less detailed in examples. **Bard**, **Cohere Command**, and **Meta Llama** gave accurate responses but lacked depth or simplicity for student-level understanding.

# Result : 
The Prompt for the above problem statement executed successfully.
